{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shNK0hXfwCTI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering + GNN"
      ],
      "metadata": {
        "id": "hALYDv1UzobA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df_movie = pd.read_csv('/content/movie_df_comm.csv')\n",
        "df_book = pd.read_csv('/content/book_df_comm.csv')\n",
        "\n",
        "# Concatenate dataframes\n",
        "df = pd.concat([df_book, df_movie], ignore_index=True)\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "df = df.drop_duplicates(subset=['reviewerID', 'asin'])\n",
        "df = df.dropna(subset=['reviewerID', 'asin', 'overall'])\n",
        "\n",
        "# Generate unique user and item IDs\n",
        "df['user_id'] = df['reviewerID'].astype('category').cat.codes\n",
        "df['item_id'] = df['asin'].astype('category').cat.codes\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# After splitting the data into train and test sets, reindex user_id and item_id\n",
        "train_df['user_id'] = train_df['user_id'].astype('category').cat.codes\n",
        "train_df['item_id'] = train_df['item_id'].astype('category').cat.codes\n",
        "test_df['user_id'] = test_df['user_id'].astype('category').cat.codes\n",
        "test_df['item_id'] = test_df['item_id'].astype('category').cat.codes\n",
        "\n",
        "# Update num_users and num_items based on the reindexed IDs\n",
        "num_users = train_df['user_id'].nunique()\n",
        "num_items = train_df['item_id'].nunique()\n",
        "\n",
        "# Update the interaction matrix with reindexed IDs\n",
        "interaction_matrix = train_df.pivot(index='user_id', columns='item_id', values='overall').fillna(0)\n",
        "\n",
        "# Cluster users and items\n",
        "num_clusters = 50  # Adjust based on your dataset size\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "\n",
        "# Cluster users\n",
        "user_clusters = kmeans.fit_predict(interaction_matrix)\n",
        "user_cluster_mapping = {user_id: cluster for user_id, cluster in zip(interaction_matrix.index, user_clusters)}\n",
        "train_df['user_cluster'] = train_df['user_id'].map(user_cluster_mapping)\n",
        "\n",
        "# Cluster items\n",
        "item_clusters = kmeans.fit_predict(interaction_matrix.T)\n",
        "item_cluster_mapping = {item_id: cluster for item_id, cluster in zip(interaction_matrix.columns, item_clusters)}\n",
        "train_df['item_cluster'] = train_df['item_id'].map(item_cluster_mapping)\n",
        "\n",
        "# Create edge index for the graph\n",
        "user_ids, item_ids = interaction_matrix.to_numpy().nonzero()\n",
        "user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
        "item_ids = torch.tensor(item_ids + num_users, dtype=torch.long)  # Offset item IDs\n",
        "edge_index = torch.stack([user_ids, item_ids], dim=0)\n",
        "\n",
        "# Normalize adjacency matrix\n",
        "from torch_geometric.utils import to_undirected, degree\n",
        "\n",
        "edge_index = to_undirected(edge_index)\n",
        "row, col = edge_index\n",
        "deg = degree(row, num_users + num_items, dtype=torch.float)\n",
        "deg_inv_sqrt = deg.pow(-0.5)\n",
        "norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "# Create PyTorch Geometric data object\n",
        "data = Data(edge_index=edge_index, edge_weight=norm)\n",
        "\n",
        "# Add cluster features to the graph\n",
        "user_cluster_features = torch.zeros(num_users, num_clusters)\n",
        "user_cluster_features[torch.arange(num_users), torch.tensor(user_clusters)] = 1\n",
        "\n",
        "item_cluster_features = torch.zeros(num_items, num_clusters)\n",
        "item_cluster_features[torch.arange(num_items), torch.tensor(item_clusters)] = 1\n",
        "\n",
        "# Combine user and item features\n",
        "cluster_features = torch.cat([user_cluster_features, item_cluster_features], dim=0)\n",
        "data.x = cluster_features  # Add cluster features to the graph\n",
        "\n",
        "# Define the LightGNN model with cluster features\n",
        "class LightGNN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64, num_clusters=50):\n",
        "        super(LightGNN, self).__init__()\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "        self.cluster_proj = nn.Linear(num_clusters, embedding_dim)  # Project cluster features to embedding space\n",
        "        self.gcn = GCNConv(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        user_emb = self.user_embedding.weight\n",
        "        item_emb = self.item_embedding.weight\n",
        "        embeddings = torch.cat([user_emb, item_emb], dim=0)\n",
        "\n",
        "        # Add cluster features to embeddings\n",
        "        cluster_emb = self.cluster_proj(x)\n",
        "        embeddings += cluster_emb\n",
        "\n",
        "        # Pass through GCN\n",
        "        embeddings = self.gcn(embeddings, edge_index)\n",
        "\n",
        "        # Split embeddings back into users and items\n",
        "        user_embeddings = embeddings[:self.user_embedding.num_embeddings]\n",
        "        item_embeddings = embeddings[self.user_embedding.num_embeddings:]\n",
        "\n",
        "        return user_embeddings, item_embeddings\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = LightGNN(num_users, num_items, embedding_dim=64, num_clusters=num_clusters)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    user_embeddings, item_embeddings = model(data.x, data.edge_index)\n",
        "\n",
        "    # Sample positive and negative interactions\n",
        "    pos_interactions = train_df[['user_id', 'item_id']].values\n",
        "    neg_items = torch.randint(0, num_items, (pos_interactions.shape[0],))\n",
        "\n",
        "    # Get scores for positive and negative interactions\n",
        "    pos_scores = torch.sum(user_embeddings[pos_interactions[:, 0]] * item_embeddings[pos_interactions[:, 1]], dim=1)\n",
        "    neg_scores = torch.sum(user_embeddings[pos_interactions[:, 0]] * item_embeddings[neg_items], dim=1)\n",
        "\n",
        "    # Compute BPR loss\n",
        "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item()}\")\n",
        "\n",
        "# Extract user and item embeddings from the LightGNN model\n",
        "user_embeddings, item_embeddings = model(data.x, data.edge_index)\n",
        "user_embeddings = user_embeddings.detach().numpy()  # Shape: (num_users, embedding_dim)\n",
        "item_embeddings = item_embeddings.detach().numpy()  # Shape: (num_items, embedding_dim)\n",
        "\n",
        "# Generate recommendations for each user\n",
        "def recommend_items(user_id, user_embeddings, item_embeddings, top_n=10):\n",
        "    user_embedding = user_embeddings[user_id]  # Shape: (embedding_dim,)\n",
        "    scores = np.dot(item_embeddings, user_embedding)  # Dot product between item embeddings and user embedding\n",
        "    top_items = np.argsort(scores)[-top_n:]  # Get top N items with highest scores\n",
        "    return top_items\n",
        "\n",
        "# Example: Recommend items for user 0\n",
        "recommended_items = recommend_items(0, user_embeddings, item_embeddings)\n",
        "print(\"Recommended items for user 0:\", recommended_items)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W6M17S80j2s",
        "outputId": "410a56e3-68c1-478e-e0df-8ce674bcc15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 0.05831856653094292\n",
            "Epoch 10: Loss 0.007918637245893478\n",
            "Epoch 20: Loss 0.0026469137519598007\n",
            "Epoch 30: Loss 0.005871028173714876\n",
            "Epoch 40: Loss 0.00162618572358042\n",
            "Epoch 50: Loss 0.0019342484883964062\n",
            "Epoch 60: Loss 0.0019004239002242684\n",
            "Epoch 70: Loss 0.0022806404158473015\n",
            "Epoch 80: Loss 0.0017112812492996454\n",
            "Epoch 90: Loss 0.0026080275420099497\n",
            "Recommended items for user 0: [ 639 1058 3873 2041 2889 2965 2927 3238 2237 2491]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "def evaluate_recommendations(test_df, user_embeddings, item_embeddings, top_n=10):\n",
        "    hit_rate = 0\n",
        "    ndcg = 0\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    total_users = test_df['user_id'].nunique()\n",
        "\n",
        "    for user_id in test_df['user_id'].unique():\n",
        "        # Get ground truth items for the user\n",
        "        ground_truth = test_df[test_df['user_id'] == user_id]['item_id'].values\n",
        "\n",
        "        # Generate recommendations for the user\n",
        "        recommended_items = recommend_items(user_id, user_embeddings, item_embeddings, top_n)\n",
        "\n",
        "        # Skip evaluation if no ground truth exists\n",
        "        if len(ground_truth) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate Hit Rate\n",
        "        if len(np.intersect1d(recommended_items, ground_truth)) > 0:\n",
        "            hit_rate += 1\n",
        "\n",
        "        # Calculate NDCG\n",
        "        relevance = np.isin(recommended_items, ground_truth).astype(int)\n",
        "        if np.sum(relevance) > 0:\n",
        "            ndcg += ndcg_score([relevance], [np.ones_like(relevance)], k=top_n)\n",
        "\n",
        "        # Calculate Precision and Recall\n",
        "        true_positives = len(np.intersect1d(recommended_items, ground_truth))\n",
        "        precision += true_positives / top_n\n",
        "        recall += true_positives / len(ground_truth)\n",
        "\n",
        "    # Average metrics across all users\n",
        "    hit_rate /= total_users\n",
        "    ndcg /= total_users\n",
        "    precision /= total_users\n",
        "    recall /= total_users\n",
        "\n",
        "    return hit_rate, ndcg, precision, recall\n",
        "\n",
        "# Example usage\n",
        "hit_rate, ndcg, precision, recall = evaluate_recommendations(test_df, user_embeddings, item_embeddings, top_n=10)\n",
        "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
        "print(f\"NDCG: {ndcg:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9gvpjba1X3e",
        "outputId": "5032d81c-a139-4042-f87f-830cf7ed0b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Rate: 0.0026\n",
            "NDCG: 0.0012\n",
            "Precision: 0.0003\n",
            "Recall: 0.0017\n"
          ]
        }
      ]
    }
  ]
}