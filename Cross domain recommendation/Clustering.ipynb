{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJNSZHaAu5NQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "l-nITmZgylhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "df_movie = pd.read_csv('/content/movie_df_comm.csv')\n",
        "df_book = pd.read_csv('/content/book_df_comm.csv')\n",
        "\n",
        "# Concatenate dataframes\n",
        "df = pd.concat([df_book, df_movie], ignore_index=True)\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "df = df.drop_duplicates(subset=['reviewerID', 'asin'])\n",
        "df = df.dropna(subset=['reviewerID', 'asin', 'overall'])\n",
        "\n",
        "# Generate unique user and item IDs\n",
        "df['user_id'] = df['reviewerID'].astype('category').cat.codes\n",
        "df['item_id'] = df['asin'].astype('category').cat.codes\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the dataframes for later use\n",
        "train_df.to_pickle(\"train_df.pkl\")\n",
        "test_df.to_pickle(\"test_df.pkl\")"
      ],
      "metadata": {
        "id": "-qUQYqHZyuIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df_movie = pd.read_csv('/content/movie_df_comm.csv')\n",
        "df_book = pd.read_csv('/content/book_df_comm.csv')\n",
        "\n",
        "# Concatenate dataframes\n",
        "df = pd.concat([df_book, df_movie], ignore_index=True)\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "df = df.drop_duplicates(subset=['reviewerID', 'asin'])\n",
        "df = df.dropna(subset=['reviewerID', 'asin', 'overall'])\n",
        "\n",
        "# Generate unique user and item IDs\n",
        "df['user_id'] = df['reviewerID'].astype('category').cat.codes\n",
        "df['item_id'] = df['asin'].astype('category').cat.codes\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the dataframes for later use\n",
        "train_df.to_pickle(\"train_df.pkl\")\n",
        "test_df.to_pickle(\"test_df.pkl\")\n",
        "\n",
        "# Create user-item interaction matrix\n",
        "interaction_matrix = train_df.pivot(index='user_id', columns='item_id', values='overall').fillna(0)\n",
        "\n",
        "# Cluster users based on their interactions\n",
        "num_clusters = 50  # Adjust based on your dataset size\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "user_clusters = kmeans.fit_predict(interaction_matrix)\n",
        "\n",
        "# Create a mapping from user_id to cluster\n",
        "user_cluster_mapping = {user_id: cluster for user_id, cluster in zip(interaction_matrix.index, user_clusters)}\n",
        "\n",
        "# Map user clusters to train_df\n",
        "train_df['user_cluster'] = train_df['user_id'].map(user_cluster_mapping)\n",
        "\n",
        "# Cluster items based on their interactions (transpose the interaction matrix)\n",
        "item_clusters = kmeans.fit_predict(interaction_matrix.T)\n",
        "\n",
        "# Create a mapping from item_id to cluster\n",
        "item_cluster_mapping = {item_id: cluster for item_id, cluster in zip(interaction_matrix.columns, item_clusters)}\n",
        "\n",
        "# Map item clusters to train_df\n",
        "train_df['item_cluster'] = train_df['item_id'].map(item_cluster_mapping)\n",
        "\n",
        "# Function to recommend items\n",
        "def recommend_items(user_id, train_df, top_n=10):\n",
        "    # Check if the user exists in the training data\n",
        "    if user_id not in train_df['user_id'].values:\n",
        "        return []  # Return an empty list if the user is not found\n",
        "\n",
        "    # Get the user's cluster\n",
        "    user_cluster = train_df[train_df['user_id'] == user_id]['user_cluster'].values[0]\n",
        "\n",
        "    # Find items in the same cluster as the user\n",
        "    cluster_items = train_df[train_df['item_cluster'] == user_cluster]['item_id'].unique()\n",
        "\n",
        "    # Rank items by average rating in the cluster\n",
        "    item_ratings = train_df[train_df['item_id'].isin(cluster_items)].groupby('item_id')['overall'].mean()\n",
        "    top_items = item_ratings.sort_values(ascending=False).index[:top_n]\n",
        "\n",
        "    return top_items\n",
        "\n",
        "# Function to evaluate recommendations\n",
        "def evaluate_recommendations(test_df, train_df, top_n=10):\n",
        "    hit_rate = 0\n",
        "    ndcg = 0\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    total_users = test_df['user_id'].nunique()\n",
        "\n",
        "    for user_id in test_df['user_id'].unique():\n",
        "        # Get ground truth items for the user\n",
        "        ground_truth = test_df[test_df['user_id'] == user_id]['item_id'].values\n",
        "\n",
        "        # Generate recommendations for the user\n",
        "        recommended_items = recommend_items(user_id, train_df, top_n)\n",
        "\n",
        "        # Skip evaluation if no recommendations are generated\n",
        "        if len(recommended_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate Hit Rate\n",
        "        if len(np.intersect1d(recommended_items, ground_truth)) > 0:\n",
        "            hit_rate += 1\n",
        "\n",
        "        # Calculate NDCG\n",
        "        relevance = np.isin(recommended_items, ground_truth).astype(int)\n",
        "        if np.sum(relevance) > 0:\n",
        "            ndcg += ndcg_score([relevance], [np.ones_like(relevance)], k=top_n)\n",
        "\n",
        "        # Calculate Precision and Recall\n",
        "        true_positives = len(np.intersect1d(recommended_items, ground_truth))\n",
        "        precision += true_positives / top_n\n",
        "        recall += true_positives / len(ground_truth)\n",
        "\n",
        "    # Average metrics across all users\n",
        "    hit_rate /= total_users\n",
        "    ndcg /= total_users\n",
        "    precision /= total_users\n",
        "    recall /= total_users\n",
        "\n",
        "    return hit_rate, ndcg, precision, recall\n",
        "\n",
        "# Example usage\n",
        "hit_rate, ndcg, precision, recall = evaluate_recommendations(test_df, train_df, top_n=10)\n",
        "print(f\"Hit Rate: {hit_rate:.4f}\")\n",
        "print(f\"NDCG: {ndcg:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sCUQSTLywzC",
        "outputId": "60444cae-721a-4c4d-fcbf-39453dc4d851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit Rate: 0.0009\n",
            "NDCG: 0.0007\n",
            "Precision: 0.0001\n",
            "Recall: 0.0009\n"
          ]
        }
      ]
    }
  ]
}